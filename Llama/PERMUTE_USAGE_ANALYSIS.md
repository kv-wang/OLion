# convert_to_hf.py ä¸­ permute å‡½æ•°ä½¿ç”¨åˆ†æ

## è°ƒç”¨é“¾åˆ†æ

### 1. convert_to_hf.py ä¸»æµç¨‹
```python
def convert_to_hf(input_dir, output_dir, model_name, model_flavor, hf_assets_path):
    # 1. è·å–è®­ç»ƒè§„æ ¼
    train_spec = train_spec_module.get_train_spec(model_name)  # llama3
    
    # 2. åˆ›å»ºçŠ¶æ€å­—å…¸é€‚é…å™¨
    sd_adapter = train_spec.state_dict_adapter(model_args, hf_assets_path)
    # sd_adapter = Llama3StateDictAdapter(model_args, hf_assets_path)
    
    # 3. åŠ è½½checkpoint
    state_dict = model._get_state_dict()
    dcp.load(state_dict, checkpoint_id=input_dir)
    
    # 4. å…³é”®è½¬æ¢æ­¥éª¤ - è°ƒç”¨ to_hf æ–¹æ³•
    hf_state_dict = sd_adapter.to_hf(state_dict)  # â† è¿™é‡Œä¼šä½¿ç”¨ _permute
```

### 2. Llama3StateDictAdapter.to_hf() æ–¹æ³•
```python
def to_hf(self, state_dict: dict[str, Any]) -> dict[str, Any]:
    # åè½¬æ˜ å°„å…³ç³»
    to_hf_map = {v: k for k, v in self.from_hf_map.items()}
    
    for key, value in state_dict.items():
        if "layers" in key:
            abstract_key = re.sub(r"(\d+)", "{}", key, count=1)
            
            # ğŸ”¥ å…³é”®ï¼šå¤„ç† q_proj æƒé‡æ—¶ä½¿ç”¨ _permute
            if abstract_key == "layers.{}.attention.wq.weight":
                value = self._permute(value, n_heads)  # â† ä½¿ç”¨ _permute
            
            # ğŸ”¥ å…³é”®ï¼šå¤„ç† k_proj æƒé‡æ—¶ä½¿ç”¨ _permute  
            if abstract_key == "layers.{}.attention.wk.weight":
                key_value_dim = head_dim * n_kv_heads
                value = self._permute(value, n_kv_heads, key_value_dim, dim)  # â† ä½¿ç”¨ _permute
```

### 3. _permute å‡½æ•°çš„ä½œç”¨
```python
def _permute(self, w, n_heads_arg, dim1=None, dim2=None):
    """å°† TorchTitan æ ¼å¼çš„æƒé‡è½¬æ¢ä¸º HuggingFace æ ¼å¼"""
    return (
        w.view(n_heads_arg, dim1 // n_heads_arg // 2, 2, dim2)
        .transpose(1, 2)
        .reshape(dim1, dim2)
        .clone()
    )
```

## å…·ä½“ä½¿ç”¨åœºæ™¯

### âœ… **ä¼šä½¿ç”¨ _permute çš„æƒ…å†µ**
å½“ `convert_to_hf.py` è¿è¡Œæ—¶ï¼Œå¯¹äºä»¥ä¸‹å‚æ•°ä¼šè°ƒç”¨ `_permute` å‡½æ•°ï¼š

1. **Query æŠ•å½±æƒé‡** (`wq.weight`):
   ```python
   # åŸå§‹é”®: layers.0.attention.wq.weight
   # è½¬æ¢å: model.layers.0.self_attn.q_proj.weight
   value = self._permute(value, n_heads)
   ```

2. **Key æŠ•å½±æƒé‡** (`wk.weight`):
   ```python
   # åŸå§‹é”®: layers.0.attention.wk.weight  
   # è½¬æ¢å: model.layers.0.self_attn.k_proj.weight
   value = self._permute(value, n_kv_heads, key_value_dim, dim)
   ```

### âŒ **ä¸ä¼šä½¿ç”¨ _reverse_permute çš„æƒ…å†µ**
`_reverse_permute` å‡½æ•°åªåœ¨ `from_hf()` æ–¹æ³•ä¸­ä½¿ç”¨ï¼Œç”¨äºå°† HuggingFace æ ¼å¼è½¬æ¢å› TorchTitan æ ¼å¼ï¼š

```python
def from_hf(self, hf_state_dict: dict[str, Any]) -> dict[str, Any]:
    # åªåœ¨ from_hf ä¸­ä½¿ç”¨ï¼Œconvert_to_hf.py ä¸ä¼šè°ƒç”¨
    if abstract_key == "model.layers.{}.self_attn.q_proj.weight":
        value = self._reverse_permute(value, n_heads)  # â† convert_to_hf.py ä¸ä¼šæ‰§è¡Œè¿™é‡Œ
```

## ä¸ºä»€ä¹ˆéœ€è¦ permuteï¼Ÿ

### RoPE å®ç°å·®å¼‚
1. **TorchTitan**: ä½¿ç”¨åŸå§‹çš„ Llama RoPE å®ç°
2. **HuggingFace**: ä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„ RoPE å®ç°

### æƒé‡æ’åˆ—ä¸åŒ
- **TorchTitan**: `[head1_dim1, head1_dim2, head2_dim1, head2_dim2, ...]`
- **HuggingFace**: `[head1_dim1, head2_dim1, head1_dim2, head2_dim2, ...]`

### ç½®æ¢çš„ä½œç”¨
é€šè¿‡ `_permute` å‡½æ•°é‡æ–°æ’åˆ—æƒé‡ç»´åº¦ï¼Œç¡®ä¿ï¼š
- RoPE è®¡ç®—åœ¨ä¸¤ä¸ªæ¡†æ¶é—´ä¿æŒä¸€è‡´
- æ¨¡å‹æ€§èƒ½ä¸å—å½±å“
- ç”Ÿæˆç»“æœæ­£ç¡®

## æ€»ç»“

**å›ç­”ä½ çš„é—®é¢˜**ï¼š

âœ… **æ˜¯çš„**ï¼Œåœ¨è¿è¡Œ `convert_to_hf.py` æ—¶ä¼šä½¿ç”¨ `state_dict_adapter.py` ä¸­çš„ `_permute` å‡½æ•°

âŒ **ä¸ä¼š**ä½¿ç”¨ `_reverse_permute` å‡½æ•°ï¼ˆé‚£æ˜¯ç”¨äºåå‘è½¬æ¢çš„ï¼‰

**å…·ä½“ä½¿ç”¨åœºæ™¯**ï¼š
- è½¬æ¢ `layers.{}.attention.wq.weight` æ—¶ä½¿ç”¨ `_permute(value, n_heads)`
- è½¬æ¢ `layers.{}.attention.wk.weight` æ—¶ä½¿ç”¨ `_permute(value, n_kv_heads, key_value_dim, dim)`

**ç›®çš„**ï¼šè§£å†³ TorchTitan å’Œ HuggingFace åœ¨ RoPE å®ç°ä¸Šçš„å·®å¼‚ï¼Œç¡®ä¿è½¬æ¢åçš„æ¨¡å‹èƒ½æ­£ç¡®å·¥ä½œã€‚
